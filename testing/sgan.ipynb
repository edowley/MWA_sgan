{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from classifiers import Train_SGAN_DM_Curve, Train_SGAN_Freq_Phase, Train_SGAN_Time_Phase, Train_SGAN_Pulse_Profile\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-20 13:56:49.626183: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-20 13:56:49.626211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "base_dir = '/home/isaaccolleran/Documents/sgan/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading training, validation and unlabelled file data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from load_MWA_data import get_files_list, load_feature_datasets\n",
    "\n",
    "# labelled training files\n",
    "path_to_data = base_dir + 'MWA_cands/'\n",
    "pfd_files, pfd_labels = get_files_list(path_to_data, 'training_labels.csv')\n",
    "\n",
    "# validation files\n",
    "path_to_validation = base_dir + 'MWA_validation/'\n",
    "validation_files, validation_labels = get_files_list(path_to_validation, 'validation_labels.csv')\n",
    "\n",
    "# unlabelled training files\n",
    "path_to_unlabelled = base_dir + 'MWA_unlabelled_cands/'\n",
    "unlabelled_files, unlabelled_labels = get_files_list(path_to_unlabelled, 'training_labels.csv')\n",
    "\n",
    "# loading the physical data\n",
    "dm_curve_data, freq_phase_data, pulse_profile_data, time_phase_data = load_feature_datasets(pfd_files)\n",
    "validation_dm_curve_data, validation_freq_phase_data, validation_pulse_profile_data, validation_time_phase_data = load_feature_datasets(validation_files)\n",
    "unlabelled_dm_curve_data, unlabelled_freq_phase_data, unlabelled_pulse_profile_data, unlabelled_time_phase_data = load_feature_datasets(unlabelled_files)\n",
    "\n",
    "# combining labels and data\n",
    "dm_curve_dataset = [dm_curve_data, pfd_labels]\n",
    "dm_curve_validation_dataset = [validation_dm_curve_data, validation_labels]\n",
    "dm_curve_unlabelled_dataset = [unlabelled_dm_curve_data, unlabelled_labels]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Functions (only for DM curve)\n",
    "These functions will be useful for the training procedure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate_real_samples is a function that randomly selects n_samples from the dataset. It also has the ability to apply label smoothing to the labels of the dataset\n",
    "\n",
    "train_c_model is a function that can be called to train exclusively the supervised discriminator. It is a very specific function that will only ever needed to be called for training the c_model. All other functions, unless specified specifically, have been generalised"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shared Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def generate_real_samples(dataset, n_samples, noisy_labels=True):\n",
    "    # dataset is fed in as [x_data, labels]\n",
    "    # it is also important to differentiate between labels and y values\n",
    "    #    -> label = pulsar/non-pulsar\n",
    "    #    -> y = real/generated\n",
    "    \n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "#     print(images.shape, labels.shape)\n",
    "    \n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    \n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    \n",
    "    # generate class labels\n",
    "    if noisy_labels:\n",
    "        y = np.random.uniform(0.9, 1, (n_samples, 1))\n",
    "    else:\n",
    "        y = np.ones((n_samples, 1))\n",
    "     \n",
    "    return [X, labels], y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    z_input = np.random.randn(latent_dim * n_samples)\n",
    "\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = z_input.reshape(n_samples, latent_dim)\n",
    "\n",
    "    return z_input\n",
    "\n",
    "## use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples, noisy_labels=True):\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "\n",
    "    # predict outputs\n",
    "    images = generator.predict(z_input)\n",
    "    \n",
    "    # create class labels\n",
    "    if noisy_labels:\n",
    "        y = np.random.uniform(0.0,0.2, (n_samples, 1))\n",
    "    else:\n",
    "        y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    return images, y\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def summarise_performance(step, c_model, validation_dataset, epoch_number, model_accuracy, save_best_model=True):\n",
    "    \n",
    "    validation_x, validation_y = validation_dataset\n",
    "    validation_x = np.array(validation_x)\n",
    "    validation_y = np.array(validation_y)\n",
    "    \n",
    "    _, acc = c_model.evaluate(validation_x, validation_y, verbose=0)\n",
    "    \n",
    "    with open('training_logs/model_performance_sgan_dm_curve.txt', 'a') as f:\n",
    "        f.write('intermediate_models/dm_curve_c_model_epoch_%d.h5' % int(epoch_number) + ',' + '%.3f' % (acc) + '\\n')\n",
    "\n",
    "    if save_best_model == True:\n",
    "        if acc > model_accuracy:\n",
    "            print('Current Model has %.3f training accuracy which is better than previous best of %.3f. Will save it as as new best model.' % (acc * 100, model_accuracy * 100 ))\n",
    "            \n",
    "            filename3 = 'MWA_best_retrained_models/dm_curve_best_discriminator_model.h5'\n",
    "            c_model.save(filename3)\n",
    "            model_accuracy = acc\n",
    "        else:\n",
    "            print('Current Model is not as good as the best model. This model will not be saved. Accuracy %.3f' % (acc * 100))\n",
    "\n",
    "        return model_accuracy, acc\n",
    "    else:\n",
    "        print('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "\n",
    "        # save the classifier model\n",
    "        filename3 = 'MWA_intermediate_models/dm_curve_c_model_epoch_%d.h5' %int(epoch_number)\n",
    "        c_model.save(filename3)\n",
    "        print('>Saved: %s, and %s' % (filename2, filename3))\n",
    "        return model_accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c_model Training Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def train_c_model(c_model, dataset, validation_dataset, batch_size, latent_dim=100, n_epochs=10):\n",
    "    # inputs:\n",
    "    #    -> c_model: the supervised discriminator model\n",
    "    #    -> dataset: numpy array containing the plot data and the labels in the form [plot_data, labels]\n",
    "    #    -> validation_dataset: the same as dataset except for the validation set\n",
    "    #    -> batch_size: int - number of samples before back-propagation is done\n",
    "    #    -> (optional) latent_dim: int - size of sample noise for generator\n",
    "    #    -> (optional) n_epoch: int - number of epochs to train for with these settings\n",
    "    \n",
    "    n_batch = batch_size\n",
    "    x, y = dataset\n",
    "    \n",
    "    # number of batches per training epoch\n",
    "    batch_per_epoch = int(np.array(x).shape[0] / n_batch)\n",
    "    \n",
    "    # calculate the number of training iterations\n",
    "    n_steps = batch_per_epoch * n_epochs\n",
    "    \n",
    "    print('n_epochs=%d, n_batch=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, batch_per_epoch, n_steps))\n",
    "    \n",
    "    # initial declarations before training loop\n",
    "    epoch_number = 0\n",
    "    model_accuracy = 0.0\n",
    "    accuracies = np.zeros((n_epochs))\n",
    "    testing_acc = np.zeros((n_epochs))\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        [x_real, y_real], _ = generate_real_samples(dataset, batch_size, noisy_labels=True)\n",
    "        x_real = np.reshape(x_real, (batch_size, 60, 1))\n",
    "        c_loss, c_acc = c_model.train_on_batch(x_real, y_real)\n",
    "        \n",
    "        # print('>%d, c[loss = %.3f, accuracy = %.0f]' % (i+1, c_loss, c_acc*100))\n",
    "\n",
    "        # if i % batch_per_epoch == 0:\n",
    "        #     print(x_real.shape, y_real.shape)\n",
    "        \n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % (batch_per_epoch * 1) == 0:\n",
    "            epoch_number += 1\n",
    "            model_accuracy, this_acc = summarise_performance(i, c_model, validation_dataset, epoch_number, model_accuracy)\n",
    "            accuracies[epoch_number - 1] = this_acc\n",
    "\n",
    "            _, test_acc = c_model.evaluate(x_real, y_real, verbose=0)\n",
    "            testing_acc[epoch_number - 1] = test_acc\n",
    "\n",
    "            \n",
    "    \n",
    "    # plotting the accuracies\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(n_epochs)), accuracies, list(range(n_epochs)), testing_acc)\n",
    "    plt.xlabel('Epoch no.')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Batch size = {}, num epochs = {}'.format(batch_size, n_epochs))\n",
    "    plt.legend(['validation', 'training'])\n",
    "    plt.savefig('retrain_sgan.png')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Model Instances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "batch_size = 16\n",
    "\n",
    "dm_curve_instance = Train_SGAN_DM_Curve(dm_curve_data, pfd_labels, validation_dm_curve_data, validation_labels, unlabelled_dm_curve_data, unlabelled_labels, batch_size)\n",
    "pulse_profile_instance = Train_SGAN_Pulse_Profile(pulse_profile_data, pfd_labels, validation_pulse_profile_data, validation_labels, unlabelled_pulse_profile_data, unlabelled_labels, batch_size)\n",
    "freq_phase_instance = Train_SGAN_Freq_Phase(freq_phase_data, pfd_labels, validation_freq_phase_data, validation_labels, unlabelled_freq_phase_data, unlabelled_labels, batch_size)\n",
    "time_phase_instance = Train_SGAN_Time_Phase(time_phase_data, pfd_labels, validation_time_phase_data, validation_labels, unlabelled_time_phase_data, unlabelled_labels, batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retraining from scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# retraining freq_phase model ##################\n",
    "# d_model, c_model = freq_phase_instance.define_discriminator()\n",
    "# generator = freq_phase_instance.define_generator()\n",
    "# gan = freq_phase_instance.define_gan(generator, d_model)\n",
    "# freq_phase_instance.train(generator, d_model, c_model, gan, n_epochs=25)\n",
    "\n",
    "# retraining time_phase model ##################\n",
    "# d_model, c_model = time_phase_instance.define_discriminator()\n",
    "# generator = time_phase_instance.define_generator()\n",
    "# gan = time_phase_instance.define_gan(generator, d_model)\n",
    "# time_phase_instance.train(generator, d_model, c_model, gan, n_epochs=25)\n",
    "\n",
    "# retraining dm_curve model ####################\n",
    "# d_model, c_model = dm_curve_instance.define_discriminator()\n",
    "# generator = freq_phase_instance.define_generator()\n",
    "# gan = freq_phase_instance.define_gan(generator, d_model)\n",
    "# freq_phase_instance.train(generator, d_model, c_model, gan, n_epochs=25)\n",
    "\n",
    "# retraining pulse_profile model ################\n",
    "# d_model, c_model = pulse_profile_instance.define_discriminator()\n",
    "# generator = freq_phase_instance.define_generator()\n",
    "# gan = freq_phase_instance.define_gan(generator, d_model)\n",
    "# freq_phase_instance.train(generator, d_model, c_model, gan, n_epochs=25)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = load_model('MWA_best_retrained_models/pre-trained/dm_curve_best_discriminator_model.h5')\n",
    "# print(model.summary())\n",
    "a = model.predict(validation_dm_curve_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-20 13:59:20.594683: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 13:59:20.613855: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3193480000 Hz\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# print(freq_phase_data.shape)\n",
    "# fig = plt.figure()\n",
    "# plt.imshow(freq_phase_data[4], cmap='gray')\n",
    "\n",
    "# freq_phase_instance.train(generator, d_model, c_model, gan, n_epochs=25)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating GAN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "dir_to_model = base_dir + 'best_retrained_models/'\n",
    "\n",
    "# loading generators\n",
    "dm_curve_generator = load_model(dir_to_model + 'dm_curve_best_generator_model.h5')\n",
    "time_phase_generator = load_model(dir_to_model + 'time_phase_best_generator_model.h5')\n",
    "freq_phase_generator = load_model(dir_to_model + 'freq_phase_best_generator_model.h5')\n",
    "pulse_profile_generator = load_model(dir_to_model + 'pulse_profile_best_generator_model.h5')\n",
    "\n",
    "# creating d_model discriminators\n",
    "dm_curve_discriminator, _ = dm_curve_instance.define_discriminator()\n",
    "pulse_profile_discriminator, _ = pulse_profile_instance.define_discriminator()\n",
    "time_phase_discriminator, _ = time_phase_instance.define_discriminator()\n",
    "freq_phase_discriminator, _ = freq_phase_instance.define_discriminator()\n",
    "\n",
    "# loading the saved weights\n",
    "dm_curve_discriminator.load_weights(dir_to_model + 'dm_curve_best_discriminator_model.h5')\n",
    "pulse_profile_discriminator.load_weights(dir_to_model + 'pulse_profile_best_discriminator_model.h5')\n",
    "time_phase_discriminator.load_weights(dir_to_model + 'time_phase_best_discriminator_model.h5')\n",
    "freq_phase_discriminator.load_weights(dir_to_model + 'freq_phase_best_discriminator_model.h5')\n",
    "\n",
    "# creating the GANs\n",
    "dm_curve_gan = dm_curve_instance.define_gan(dm_curve_generator, dm_curve_discriminator)\n",
    "pulse_profile_gan = pulse_profile_instance.define_gan(pulse_profile_generator, pulse_profile_discriminator)\n",
    "time_phase_gan = freq_phase_instance.define_gan(time_phase_generator, time_phase_discriminator)\n",
    "freq_phase_gan = time_phase_instance.define_gan(freq_phase_generator, freq_phase_discriminator)\n",
    "\n",
    "# setting all layers except the dense layers to not trainable\n",
    "for l in (dm_curve_discriminator.layers + time_phase_discriminator.layers + freq_phase_discriminator.layers + pulse_profile_discriminator.layers):\n",
    "# for loop combines all the layers into one so that we dont need 4 separate loops\n",
    "    if not l.name.startswith('dense'):\n",
    "        l.trainable = False\n",
    "    # print(l.name, l.trainable)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Pretrained Discrimators\n",
    "\n",
    "Within this cell, all of the layers that aren't dense layers are changed to not trainable. We do this because we only want to update the weights of the densely connected layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "dir_to_model = base_dir + 'best_retrained_models/'\n",
    "\n",
    "# loading the models from the above\n",
    "dm_curve_model = load_model(dir_to_model + 'dm_curve_best_discriminator_model.h5')\n",
    "# dm_curve_model = load_model(base_dir + 'MWA_best_retrained_models/93.3_learn2e-5beta0.99_dm_curve_best_discriminator_model.h5')\n",
    "time_phase_model = load_model(dir_to_model + 'time_phase_best_discriminator_model.h5')\n",
    "freq_phase_model = load_model(dir_to_model + 'freq_phase_best_discriminator_model.h5')\n",
    "pulse_profile_model = load_model(dir_to_model + 'pulse_profile_best_discriminator_model.h5')\n",
    "\n",
    "# setting all layers except the dense layers to not trainable\n",
    "for l in (dm_curve_model.layers + time_phase_model.layers + freq_phase_model.layers + pulse_profile_model.layers):\n",
    "# for loop combines all the layers into one so that we dont need 4 separate loops\n",
    "    if not l.name.startswith('dense'):\n",
    "        l.trainable = False\n",
    "    # print(l.name, l.trainable)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Updating optimisers for the models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "opt1 = tf.optimizers.Adam(learning_rate=5e-2, beta_1=0.999) # c discriminator\n",
    "# opt1 = tf.optimizers.Adam(learning_rate=0.01, beta_1=0.99) # c discriminator\n",
    "opt2 = tf.optimizers.Adam(learning_rate=5e-2, beta_1=0.999) # d discriminator\n",
    "opt3 = tf.optimizers.Adam(learning_rate=5e-2, beta_1=0.999) # gan\n",
    "\n",
    "# dm_curve_model.optimizer = opt1\n",
    "# dm_curve_discriminator.optimizer = opt2\n",
    "# dm_curve_gan.optimizer = opt3\n",
    "\n",
    "freq_phase_model.optimizer = opt1\n",
    "freq_phase_discriminator.optimizer = opt2\n",
    "freq_phase_gan.optimizer = opt3\n",
    "\n",
    "# dm_curve_model.optimizer = opt1\n",
    "# dm_curve_discriminator.optimizer = opt2\n",
    "# dm_curve_gan.optimizer = opt3\n",
    "\n",
    "# dm_curve_model.optimizer = opt1\n",
    "# dm_curve_discriminator.optimizer = opt2\n",
    "# dm_curve_gan.optimizer = opt3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# print(dm_curve_model.optimizer.learning_rate, dm_curve_model.optimizer.beta_1)\n",
    "\n",
    "# _, acc = dm_curve_model.evaluate(validation_dm_curve_data, validation_labels)\n",
    "# print(acc)\n",
    "# print(dm_curve_model.metrics_names)\n",
    "# predictions = dm_curve_model.predict(validation_dm_curve_data)\n",
    "# print(np.sum(np.sum(predictions, axis=1)))\n",
    "# print(predictions)\n",
    "# after = np.rint(predictions)\n",
    "# after = np.argmax(after, axis=1)\n",
    "# after = np.reshape(after, len(after))\n",
    "# print([after, validation_labels])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# print(dm_curve_dataset[0].shape, dm_curve_dataset[1].shape)\n",
    "# this = dm_curve_dataset[0][413, :]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(this)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calling Training Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "batch_size = 16\n",
    "n_epoch = 25\n",
    "\n",
    "# train_c_model(dm_curve_model, dm_curve_dataset, dm_curve_validation_dataset, batch_size, n_epochs=n_epoch)\n",
    "# dm_curve_instance.train(dm_curve_generator, dm_curve_discriminator, dm_curve_model, dm_curve_gan, n_epochs=n_epoch, noisy_labels=True)\n",
    "freq_phase_instance.train(freq_phase_generator, freq_phase_discriminator, freq_phase_model, freq_phase_gan, n_epochs=n_epoch, noisy_labels=True)\n",
    "# pulse_profile_instance.train(pulse_profile_generator, pulse_profile_discriminator, pulse_profile_model, pulse_profile_gan, n_epochs=n_epoch, noisy_labels=True)\n",
    "# time_phase_instance.train(time_phase_generator, time_phase_discriminator, time_phase_model, time_phase_gan, n_epochs=n_epoch, noisy_labels=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch per epoch is 251\n",
      "n_epochs=25, n_batch=16, 1/2=8, b/e=251, steps=6275\n",
      "Current Model has 30.000 training accuracy which is better than previous best of 0.000. Will save it as as new best model.\n",
      "Current Model has 88.333 training accuracy which is better than previous best of 30.000. Will save it as as new best model.\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 25.000\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 38.333\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 13.333\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 41.667\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 11.667\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 43.333\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 30.000\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 41.667\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 13.333\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 53.333\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 68.333\n",
      "Current Model is not as good as the best model. This model will not be saved. Accuracy 36.667\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25d9d0c3a94a424b61770aaf1e3776c9a2e92fe9bc3b8feb3f48527beb297e99"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}